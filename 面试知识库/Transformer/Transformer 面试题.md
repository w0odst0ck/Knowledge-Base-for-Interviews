| 标记  | 题目                                                                                                                                                | 难度  | 标签                                                                                                                                                                                                                                                                                                 |
| --- | ------------------------------------------------------------------------------------------------------------------------------------------------- | --- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|     |                                                                                                                                                   |     |                                                                                                                                                                                                                                                                                                    |
|     | [聊一聊 Transformer 的架构和基本原理。](https://www.mianshiya.com/bank/1821834692534505473/question/1821834692723249153)                                      | 中等  | [Transformer](https://www.mianshiya.com/tag/Transformer)                                                                                                                                                                                                                                           |
|     | [使用 Transformer 解决了 RNN 面临的一些什么问题？](https://www.mianshiya.com/bank/1821834692534505473/question/1821834692991684610)                              | 简单  | [Transformer](https://www.mianshiya.com/tag/Transformer)<br><br>[RNN](https://www.mianshiya.com/tag/RNN)                                                                                                                                                                                           |
|     | [Transformer 的哪个部分最占用显存？](https://www.mianshiya.com/bank/1821834692534505473/question/1821834693230759937)                                        | 中等  | [Transformer](https://www.mianshiya.com/tag/Transformer)                                                                                                                                                                                                                                           |
|     | [Transformer 的位置编码是怎样的？](https://www.mianshiya.com/bank/1821834692534505473/question/1821834693532749826)                                         | 简单  | [Transformer](https://www.mianshiya.com/tag/Transformer)<br><br>[位置编码](https://www.mianshiya.com/tag/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81)                                                                                                                                                         |
|     | [Transformer 在计算 attention 的时候使用的是点乘还是加法？请说明理由。](https://www.mianshiya.com/bank/1821834692534505473/question/1821834693834739714)                 | 中等  | [Transformer](https://www.mianshiya.com/tag/Transformer)                                                                                                                                                                                                                                           |
|     | [self attention 中的 K 和 Q 是用来做什么的？](https://www.mianshiya.com/bank/1821834692534505473/question/1821834694111563778)                               | 简单  | [Transformer](https://www.mianshiya.com/tag/Transformer)<br><br>[自注意力机制](https://www.mianshiya.com/tag/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6)                                                                                                                                     |
|     | [K 和 Q 可以使用同一个值通过对自身进行点乘得到吗？](https://www.mianshiya.com/bank/1821834692534505473/question/1821834694384193537)                                    | 中等  | [Transformer](https://www.mianshiya.com/tag/Transformer)                                                                                                                                                                                                                                           |
|     | [如果让 K 和 Q 变成同一个矩阵，你觉得对模型性能会带来怎样的影响？](https://www.mianshiya.com/bank/1821834692534505473/question/1821834694635851777)                            | 困难  | [Transformer](https://www.mianshiya.com/tag/Transformer)                                                                                                                                                                                                                                           |
|     | [为什么 Transformer 采用多头注意力机制？](https://www.mianshiya.com/bank/1821834692534505473/question/1821834694916870145)                                     | 中等  | [Transformer](https://www.mianshiya.com/tag/Transformer)<br><br>[自注意力机制](https://www.mianshiya.com/tag/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6)<br><br>[多头注意力机制](https://www.mianshiya.com/tag/%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6)                     |
|     | [在不考虑计算量的情况下，head 能否无限增多？](https://www.mianshiya.com/bank/1821834692534505473/question/1821834695227248642)                                       | 中等  | [Transformer](https://www.mianshiya.com/tag/Transformer)<br><br>[多头注意力机制](https://www.mianshiya.com/tag/%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6)                                                                                                                           |
|     | [在进行多头注意力的时候需要对每个 head 进行降维吗？](https://www.mianshiya.com/bank/1821834692534505473/question/1821834695491489793)                                   | 中等  | [Transformer](https://www.mianshiya.com/tag/Transformer)<br><br>[多头注意力机制](https://www.mianshiya.com/tag/%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6)                                                                                                                           |
|     | [讲一下你对 Transformer 的 Encoder 模块的理解。](https://www.mianshiya.com/bank/1821834692534505473/question/1821834695759925249)                             | 中等  | [Transformer](https://www.mianshiya.com/tag/Transformer)<br><br>[Encoder](https://www.mianshiya.com/tag/Encoder)                                                                                                                                                                                   |
|     | [Transformer 中，Decoder 阶段的多头自注意力和 Encoder 阶段的多头自注意力是相同的吗？](https://www.mianshiya.com/bank/1821834692534505473/question/1821834696015777794)       | 中等  | [Transformer](https://www.mianshiya.com/tag/Transformer)<br><br>[Encoder](https://www.mianshiya.com/tag/Encoder)<br><br>[Decoder](https://www.mianshiya.com/tag/Decoder)<br><br>[多头自注意力机制](https://www.mianshiya.com/tag/%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6) |
|     | [了解 Transformer 模型训练中的梯度裁剪（Gradient Clipping）吗？](https://www.mianshiya.com/bank/1821834692534505473/question/1821834696267436033)                 | 中等  | [Transformer](https://www.mianshiya.com/tag/Transformer)<br><br>[梯度裁剪](https://www.mianshiya.com/tag/%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AA)                                                                                                                                                         |
|     | [Transformer 为什么采用 Layer Normalization 而不是 Batch Normalization?](https://www.mianshiya.com/bank/1821834692534505473/question/1821834696569425922) | 中等  | [Transformer](https://www.mianshiya.com/tag/Transformer)<br><br>[normalization](https://www.mianshiya.com/tag/normalization)                                                                                                                                                                       |
|     | [Transformer 中的注意力遮蔽（Attention Masking）的工作原理是什么？](https://www.mianshiya.com/bank/1821834692534505473/question/1821834696833667074)                | 中等  | [Transformer](https://www.mianshiya.com/tag/Transformer)<br><br>[注意力遮蔽](https://www.mianshiya.com/tag/%E6%B3%A8%E6%84%8F%E5%8A%9B%E9%81%AE%E8%94%BD)                                                                                                                                               |
|     | [什么是自回归属性（autoregressive property）？](https://www.mianshiya.com/bank/1821834692534505473/question/1821834697093713922)                             | 中等  | [Transformer](https://www.mianshiya.com/tag/Transformer)<br><br>[自回归属性](https://www.mianshiya.com/tag/%E8%87%AA%E5%9B%9E%E5%BD%92%E5%B1%9E%E6%80%A7)                                                                                                                                               |
|     | [Transformer 中如何实现序列到序列的映射？](https://www.mianshiya.com/bank/1821834692534505473/question/1821834697366343682)                                     | 中等  | [Transformer](https://www.mianshiya.com/tag/Transformer)<br><br>[编解码器](https://www.mianshiya.com/tag/%E7%BC%96%E8%A7%A3%E7%A0%81%E5%99%A8)                                                                                                                                                         |
|     | [Transformer 中的“残差连接”可以缓解梯度消失问题吗？](https://www.mianshiya.com/bank/1821834692534505473/question/1821834697638973442)                               | 中等  | [Transformer](https://www.mianshiya.com/tag/Transformer)<br><br>[残差连接](https://www.mianshiya.com/tag/%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5)<br><br>[梯度消失](https://www.mianshiya.com/tag/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1)                                                                       |
|     | [Transformer 中，如何处理大型数据集？](https://www.mianshiya.com/bank/1821834692534505473/question/1821834697936769026)                                       | 中等  | [Transformer](https://www.mianshiya.com/tag/Transformer)                                                                                                                                                                                                                                           |
|     | [Transformer 模型训练完成后，如何评估其性能和效果？](https://www.mianshiya.com/bank/1821834692534505473/question/1821834698230370305)                                | 中等  | [Transformer](https://www.mianshiya.com/tag/Transformer)                                                                                                                                                                                                                                           |
|     | [Transformer 模型的性能瓶颈在哪？](https://www.mianshiya.com/bank/1821834692534505473/question/1821834698599469057)                                         | 中等  | [Transformer](https://www.mianshiya.com/tag/Transformer)                                                                                                                                                                                                                                           |
|     | [你觉得可以怎样缓解这个性能瓶颈？](https://www.mianshiya.com/bank/1821834692534505473/question/1821834698867904514)                                               | 困难  | [Transformer](https://www.mianshiya.com/tag/Transformer)                                                                                                                                                                                                                                           |
|     | [Transformer 和 LLM 有哪些区别](https://www.mianshiya.com/bank/1821834692534505473/question/1821834699207643137)                                        | 简单  | [Transformer](https://www.mianshiya.com/tag/Transformer)<br><br>[LLM](https://www.mianshiya.com/tag/LLM)                                                                                                                                                                                           |
|     | [了解 ViT（Vision Transformer） 吗？](https://www.mianshiya.com/bank/1821834692534505473/question/1821834699513827329)                                  | 中等  | [Vision Transformer](https://www.mianshiya.com/tag/Vision%20Transformer)                                                                                                                                                                                                                           |
|     | [了解 ViLT（Vision-and-Language Transformer） 吗？](https://www.mianshiya.com/bank/1821834692534505473/question/1821834699778068481)                    | 困难  | [ViLT](https://www.mianshiya.com/tag/ViLT)                                                                                                                                                                                                                                                         |
|     | [ViLT 模型是如何将 Transformer 应用于图像识别任务的?](https://www.mianshiya.com/bank/1821834692534505473/question/1821834700050698241)                            | 中等  | [ViLT](https://www.mianshiya.com/tag/ViLT)<br><br>[图像识别](https://www.mianshiya.com/tag/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB)                                                                                                                                                                       |
|     | [chatGLM 和 GPT 在结构上有什么区别？](https://www.mianshiya.com/bank/1821834692534505473/question/1821834700398825474)                                       | 中等  | [Transformer](https://www.mianshiya.com/tag/Transformer)<br><br>[语言模型](https://www.mianshiya.com/tag/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B)                                                                                                                                                         |
