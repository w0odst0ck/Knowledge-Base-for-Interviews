| 标记  | 题目                                                                                                                                                    | 难度  | 标签                                                                                                                                                                                                                                                                                                                                             |
| --- | ----------------------------------------------------------------------------------------------------------------------------------------------------- | --- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|     |                                                                                                                                                       |     |                                                                                                                                                                                                                                                                                                                                                |
|     | [什么是词嵌入（Word Embedding）？有哪些常见的词嵌入方法？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834668647944194)                                | 中等  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[词嵌入](https://www.mianshiya.com/tag/%E8%AF%8D%E5%B5%8C%E5%85%A5)<br><br>[Word2Vec](https://www.mianshiya.com/tag/Word2Vec)                                                                                     |
|     | [是否使用 Word2Vec 训练过数据？在这个过程中，如何获取语料？如何选择超参数？语料、词表和维度大小如何确定？怎样把握训练时长？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834668916379649) | 困难  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[词嵌入](https://www.mianshiya.com/tag/%E8%AF%8D%E5%B5%8C%E5%85%A5)<br><br>[Word2Vec](https://www.mianshiya.com/tag/Word2Vec)                                                                                     |
|     | [Word2Vec 有哪些加速方法？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834669184815106)                                                  | 困难  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[词嵌入](https://www.mianshiya.com/tag/%E8%AF%8D%E5%B5%8C%E5%85%A5)<br><br>[Word2Vec](https://www.mianshiya.com/tag/Word2Vec)                                                                                     |
|     | [Word2Vec 如何获取词向量？如何评估该训练得到的词向量的好坏？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834669457444865)                                 | 中等  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[词嵌入](https://www.mianshiya.com/tag/%E8%AF%8D%E5%B5%8C%E5%85%A5)<br><br>[Word2Vec](https://www.mianshiya.com/tag/Word2Vec)<br><br>[词向量](https://www.mianshiya.com/tag/%E8%AF%8D%E5%90%91%E9%87%8F)             |
|     | [解释 hierarchical softmax 的流程，以及它有什么优点？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834669751046145)                              | 困难  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[词嵌入](https://www.mianshiya.com/tag/%E8%AF%8D%E5%B5%8C%E5%85%A5)<br><br>[Word2Vec](https://www.mianshiya.com/tag/Word2Vec)<br><br>[hierarchical softmax](https://www.mianshiya.com/tag/hierarchical%20softmax) |
|     | [说一说负采样技术在 Word2Vec 中的运用。](https://www.mianshiya.com/bank/1821834668446617601/question/1821834670023675906)                                           | 中等  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[词嵌入](https://www.mianshiya.com/tag/%E8%AF%8D%E5%B5%8C%E5%85%A5)<br><br>[Word2Vec](https://www.mianshiya.com/tag/Word2Vec)<br><br>[负采样](https://www.mianshiya.com/tag/%E8%B4%9F%E9%87%87%E6%A0%B7)             |
|     | [CBOW 和 Skip-gram 分别更适合哪些应用场景？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834670296305666)                                      | 中等  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[CBOW](https://www.mianshiya.com/tag/CBOW)<br><br>[Skip-gram](https://www.mianshiya.com/tag/Skip-gram)                                                                                                         |
|     | [说说 GloVE 技术，怎样进行训练？有哪些应用场景？相比 Word2Vec 有哪些优缺点？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834670556352514)                     | 困难  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[Word2Vec](https://www.mianshiya.com/tag/Word2Vec)<br><br>[GloVE](https://www.mianshiya.com/tag/GloVE)                                                                                                         |
|     | [说说 FastText 技术，是否比 Word2Vec 更优越？哪些情况下更适合使用 FastText?](https://www.mianshiya.com/bank/1821834668446617601/question/1821834670824787969)               | 困难  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[Word2Vec](https://www.mianshiya.com/tag/Word2Vec)<br><br>[FastText](https://www.mianshiya.com/tag/FastText)                                                                                                   |
|     | [聊一聊 ELMo 技术，它有哪些优缺点？可以做到一词多义吗？为什么？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834671101612034)                                 | 困难  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[ELMo](https://www.mianshiya.com/tag/ELMo)                                                                                                                                                                     |
|     | [说说 LSTM 的基本原理。](https://www.mianshiya.com/bank/1821834668446617601/question/1821834671391019010)                                                     | 简单  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[LSTM](https://www.mianshiya.com/tag/LSTM)                                                                                                                                                                     |
|     | [与循环神经网络（RNN）相比，LSTM 是如何解决梯度消失问题的？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834671663648770)                                  | 中等  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[LSTM](https://www.mianshiya.com/tag/LSTM)<br><br>[RNN](https://www.mianshiya.com/tag/RNN)<br><br>[梯度消失](https://www.mianshiya.com/tag/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1)                                   |
|     | [解释一个 LSTM 单元（LSTM cell）的基本组成，以及它们各自的作用。](https://www.mianshiya.com/bank/1821834668446617601/question/1821834673223929858)                            | 中等  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[LSTM](https://www.mianshiya.com/tag/LSTM)                                                                                                                                                                     |
|     | [LSTM 中，隐藏状态（hidden state）和单元状态（cell state）有什么区别？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834673496559618)                   | 中等  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[LSTM](https://www.mianshiya.com/tag/LSTM)                                                                                                                                                                     |
|     | [LSTM 和 GRU 有什么区别？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834673773383682)                                                  | 中等  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[LSTM](https://www.mianshiya.com/tag/LSTM)<br><br>[GRU](https://www.mianshiya.com/tag/GRU)                                                                                                                     |
|     | [请解释 LSTM 和 GRU 网络在处理序列数据中的应用。](https://www.mianshiya.com/bank/1821834668446617601/question/1821834674037624834)                                      | 中等  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[LSTM](https://www.mianshiya.com/tag/LSTM)<br><br>[GRU](https://www.mianshiya.com/tag/GRU)                                                                                                                     |
|     | [什么是注意力机制？它是如何改善 NLP 模型性能的？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834674301865985)                                         | 中等  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[注意力机制](https://www.mianshiya.com/tag/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6)                                                                                                                           |
|     | [请描述 BERT 模型的架构和应用场景。](https://www.mianshiya.com/bank/1821834668446617601/question/1821834674561912834)                                               | 中等  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[BERT](https://www.mianshiya.com/tag/BERT)                                                                                                                                                                     |
|     | [BERT 是如何处理自然语言文本中不常见词或者罕见词的？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834674817765378)                                       | 中等  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[BERT](https://www.mianshiya.com/tag/BERT)                                                                                                                                                                     |
|     | [Word2Vec 到 BERT 有怎样的改进？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834675128143874)                                            | 中等  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[BERT](https://www.mianshiya.com/tag/BERT)<br><br>[Word2Vec](https://www.mianshiya.com/tag/Word2Vec)                                                                                                           |
|     | [BERT 怎样进行 mask? 相比 CBOW 有什么区别？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834675400773634)                                     | 中等  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[BERT](https://www.mianshiya.com/tag/BERT)<br><br>[CBOW](https://www.mianshiya.com/tag/CBOW)                                                                                                                   |
|     | [你有什么办法可以比较好地解决 BERT 输入长度的限制？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834675660820482)                                       | 困难  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[BERT](https://www.mianshiya.com/tag/BERT)                                                                                                                                                                     |
|     | [说说你是怎样有效地优化和微调 BERT，以应对你做过的一些特定的 NLP 任务的？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834675946033154)                          | 困难  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[BERT](https://www.mianshiya.com/tag/BERT)                                                                                                                                                                     |
|     | [如何比较文本的相似度？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834676289966082)                                                        | 简单  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[文本分类](https://www.mianshiya.com/tag/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB)                                                                                                                                     |
|     | [支持向量机可以用于文本分类任务吗？若可以，请说明。](https://www.mianshiya.com/bank/1821834668446617601/question/1821834676600344578)                                          | 简单  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[文本分类](https://www.mianshiya.com/tag/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB)<br><br>[支持向量机](https://www.mianshiya.com/tag/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA)                                         |
|     | [在文本分类任务中，如何处理高维和稀疏数据？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834676906528770)                                              | 中等  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[文本分类](https://www.mianshiya.com/tag/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB)                                                                                                                                     |
|     | [在文本分类任务中，如何处理样本（类别）不平衡的问题？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834677187547137)                                         | 中等  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[文本分类](https://www.mianshiya.com/tag/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB)                                                                                                                                     |
|     | [现有文本分类算法在处理多语种文本数据时可能遭遇哪些挑战？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834677502119938)                                       | 中等  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[文本分类](https://www.mianshiya.com/tag/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB)                                                                                                                                     |
|     | [简述 Word Embedding 可以怎样运用于文本分类任务？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834677762166785)                                   | 中等  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[文本分类](https://www.mianshiya.com/tag/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB)<br><br>[Word Embedding](https://www.mianshiya.com/tag/Word%20Embedding)                                                             |
|     | [简述 LLaMA（Large Language Model Meta AI）的基本原理。](https://www.mianshiya.com/bank/1821834668446617601/question/1821834678093516802)                       | 中等  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[大语言模型](https://www.mianshiya.com/tag/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B)                                                                                                                           |
|     | [LLaMA 模型中，输入句子的长度理论上是否可以无限长？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834678357757954)                                       | 困难  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[大语言模型](https://www.mianshiya.com/tag/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B)                                                                                                                           |
|     | [了解 LLaMA 中的旋转位置编码吗？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834678626193410)                                                | 中等  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[大语言模型](https://www.mianshiya.com/tag/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B)<br><br>[旋转位置编码](https://www.mianshiya.com/tag/%E6%97%8B%E8%BD%AC%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81)                     |
|     | [LLaMA 有哪些实际应用？](https://www.mianshiya.com/bank/1821834668446617601/question/1821834678903017474)                                                     | 简单  | [自然语言处理（NLP）](https://www.mianshiya.com/tag/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%88NLP%EF%BC%89)<br><br>[大语言模型](https://www.mianshiya.com/tag/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B)                                                                                                                           |
